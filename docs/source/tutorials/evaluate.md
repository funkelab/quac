# Scoring attributions using QuAC

At this point, we will have generated a set of candidate regions, using various heuristic methods. 
Now, we will obtain the mask, counterfactual, and score for our final explanation using the `run_evaluation.py` script.

It has the following arguments, all of which are optional:
- `dataset`: Which of the datasets to run the translation on. By default this will be the "test" dataset, if that does not exist it will revert to the "validation" dataset.
- `output`: Where to store the reports generated by this evaluation. They will be further organized as sub-directories named after the different methods that we will try. By default, the attributions will go in the experiment root directory under `reports`.
- `attrs`: Where the attributions are. You should only set this if you used a custom `output` argument in the script above. By default, this is in the experiment root directory under `attributions`
- `input_fake`: Where the generated images are. You should only set this if you used a custom `output` argument in the image generation step. By default, this is in the experiment root directory under `generated_images`
- `names`: A selection of attribution methods to run evaluation on. This is a useful argument if you want to run evaluation on each method simultaneously, *e.g.* on a cluster. By default, we will sequentially run evaluation on all the methods in the `attrs` directory.

To run using the defaults, simply run: 
```{code-block} bash
python run_evaluation.py
```

You can use the following command to get help with formatting your arguments.
```{code-block} bash
python run_evaluation.py -h
```